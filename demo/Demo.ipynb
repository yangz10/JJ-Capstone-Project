{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSPSC code project - RPI Capstone\n",
    "\n",
    "**Tips**\n",
    "- This doc is only for Johnson & Johnson internal Use.\n",
    "- This doc can be divided into two parts: Brief Summary & Technical Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I : Brief Summay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All data are from Yilin Chen** (Thanks!)\n",
    "\n",
    "- Paths : All received files are under folder csvfiles (./FinalCode/defaultInput/csvFiles)\n",
    "\n",
    "**In this project, we generally have two paths to solve this problem.**\n",
    "- Traditional Model: Predict each partition of unspsc code based on the naming patterns. \n",
    "- Deep Learning Model: Hybrid model with many machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business/ questions and data understandings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Questions: Use current data from SAP to predict UNSPSC code\n",
    "- Approaches: Data Visulizations\n",
    "\n",
    "**Step 1**: Group all features by Unspsc to pick up some important features.\n",
    "- It is obvious that some features are very useful to help our model determine the specific unspsc code.\n",
    "\n",
    "**Step 2:** Choose other descriptions to train the model\n",
    "- Because we find each Unspsc may contain some repetitive words. Using NLP method will make sense.\n",
    "\n",
    "<img src=\"Unspsc.png\" width=500>\n",
    "<img src=\"Visual.png\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data cleaning and preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Model will be explained later in a separate section.**\n",
    "\n",
    "- Text Fileds: Delete comma, digits ...\n",
    "- Merge: Merge Prdha and GMDN (UNSPSC Full Data Update 2.csv with other fields to fully use the descriptions)\n",
    "- Null Values: Text model needs descriptions so that all null descriptions will be deleted. \n",
    "- Keep other features: Production Dimensions and Locations (they are related to final model)\n",
    "\n",
    "**This is an executable document. All data will be provided later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Model constructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use all fields from \"UNSPSC Full Data Update 2.csv\" and then clean all data.\n",
    "- For descriptions, 3 models have been built separately using fasttext to create new features (autoencoder).\n",
    "- Deep learning techniques (RNN - GRU) have been used for autoencoder part.\n",
    "- Merge all features together and use the tree-based model to forecast final unspsc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"model.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Code Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All data also are from \"UNSPSC Full Data Update 2.csv\"\n",
    "- Use all fields and fill missing values with -1.\n",
    "- Use random forest to predict unspsc code as a whole.\n",
    "- Important Fileds: 'Zzp3 Low Level', 'Z Gmdn Preferred Term Code', 'Prdha', 'Spart'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"code.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Model evaluations and optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"perfor.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Discussion with findings\n",
    "- High Accuracy: Overfitting Problem may exist.\n",
    "    - However, generalization ability of model is not very important because our data source may be the same.\n",
    "    - During the training process, we have adopted many methods to avoid overfitting although it seems that all these models are not real. From our perspectives, we think that the overfitting problem is not so severed. (See our demo.)\n",
    "- Code model and Text model\n",
    "    - Given limited time and resources, code model performs better.\n",
    "    - If some features like \"Z..\" can be used, code model will outperform text model. We don't realize these features are very valuable at first.\n",
    "    \n",
    "**Next Steps:**\n",
    "- Use these features and techniques in other data sources to check the stability of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II : Technical Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Code Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because code model is very simple. Here, I only provide all codes.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "products = pd.read_csv('C:/Modeling/UNSPSC Full Data Update 2.csv')\n",
    "\n",
    "# Change all objects to factors (one-hot)\n",
    "for column_name in products.columns:\n",
    "    if products[column_name].dtype == object:\n",
    "        print(column_name)\n",
    "        products[column_name] = pd.factorize(products[column_name])[0]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Replace all Null values with -1\n",
    "productsproduct .fillna(-1, inplace=True)\n",
    "\n",
    "# Create each partitions\n",
    "productsproduct ['Unspsc_str'] = products['Unspsc'].apply(str)\n",
    "products['Unspsc_str']\n",
    "#split UNSPSC into 2-digit pairs\n",
    "products['segment'] = products['Unspsc_str'].str[:2]\n",
    "products['family'] = products['Unspsc_str'].str[2:4]\n",
    "products['class'] = products['Unspsc_str'].str[4:6]\n",
    "products['commodity'] = products['Unspsc_str'].str[6:]\n",
    "\n",
    "\n",
    "# Split Train and Test Set\n",
    "train, test = train_test_split(products, test_size=0.3, random_state=0)\n",
    "train_X = train[['Zzp3 Low Level', 'Z Gmdn Preferred Term Code', 'Prdha', 'Spart']]\n",
    "train_y = train['commodity']\n",
    "test_X = test[['Zzp3 Low Level', 'Z Gmdn Preferred Term Code', 'Prdha', 'Spart']]\n",
    "test_y = test['commodity']\n",
    "\n",
    "# Build Random Forest Model\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(test_X)\n",
    "\n",
    "# Check the wrong predictions\n",
    "pd.crosstab(test_y, preds, rownames=['actual'], colnames=['preds'])\n",
    "\n",
    "# Show Model Performance\n",
    "print(accuracy_score(train_y, clf.predict(train_X)))\n",
    "print(accuracy_score(test_y, preds))\n",
    "\n",
    "# Shoe Model Feature Importance\n",
    "list(zip(train_X, clf.feature_importances_))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Code Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Main Function \n",
    "- Currently, all functions are in one model : mainFun.\n",
    "\n",
    "Tree Structure :\n",
    "\n",
    "--unspsc\n",
    "\n",
    "    |-- __init__ : Define the path\n",
    "    |-- dataPre :\n",
    "    |-- FeatureMerge\n",
    "    |-- Token\n",
    "    |-- TokenInput\n",
    "    |-- dataInput\n",
    "    |-- dataPre\n",
    "    |-- finalPrediction\n",
    "    |-- modelNetTrain\n",
    "    |-- modelPre\n",
    "    |-- modelTrain\n",
    "    |-- stagingFeature\n",
    "    |-- tokenSide\n",
    "    |-- wordEmbedding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: import mainFunc module because all related functions are saved in this module. \n",
    "``` python\n",
    "import mainFun\n",
    "```\n",
    "\n",
    "- Step 2: Initilization\n",
    "    - With all functions loaded, we can check all default paths and methods used in this module.\n",
    "``` python\n",
    "# 1.Default Path\n",
    "model = mainFun.unspsc() \n",
    "# 2.Self-defined Path\n",
    "filePath = '../FinalCode/defaultInput/csvFiles/UNSPSC Full Data Update 2.csv'\n",
    "model = mainFun.unspsc(SapPath = filePath) \n",
    "```\n",
    "\n",
    "- Step 3: Select Paths\n",
    "    * weightsPath : path for all neuron network parameters\n",
    "    * wordEmPath : embedding matrix after model training\n",
    "    * GmdnPath: : all gmdn categories\n",
    "    * PrhdaPath : current mapping file \n",
    "    * SapPath : data directely from SAP System\n",
    "    * EMBEDDING_FILE : Path for loading fasttext file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 (a) Code Demo Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Finished\n"
     ]
    }
   ],
   "source": [
    "# Initial\n",
    "import mainFun\n",
    "model = mainFun.unspsc()\n",
    "filterAll, y_train = model.dataPre()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2 (b) Code Demo Default\n",
    "- Use new documents to create new objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation Finished\n"
     ]
    }
   ],
   "source": [
    "# folder path\n",
    "rootPath = '/Users/ValarMorghulis/Johnson_Johnson/FinalCode/'\n",
    "# Data From SAP System\n",
    "SapPath = rootPath + 'defaultInput/csvFiles/UNSPSC Full Data Update 2.csv'\n",
    "# PRHDA File\n",
    "PrhdaPath= rootPath + 'defaultInput/csvFiles/Prhda.csv'\n",
    "# GMDN Description\n",
    "GmdnPath = rootPath + 'defaultInput/csvFiles/GMDN_desp.csv'\n",
    "# Fasttest WordEmbedding File\n",
    "embedPath = rootPath + 'defaultInput/wordEmbeddingMartix/crawl-300d-2M.vec'\n",
    "# All pre-trained parameters \n",
    "weightsPath = rootPath + 'defaultInput/preTrainedWeights/'\n",
    "# All pre-trained model wordembedding files\n",
    "wordEmPath = rootPath + 'defaultInput/wordEmbeddingMartix/'\n",
    "\n",
    "# Initial\n",
    "import mainFun\n",
    "model = mainFun.unspsc(SapPath = SapPath,\n",
    "                       PrhdaPath= PrhdaPath,\n",
    "                       GmdnPath = GmdnPath,\n",
    "                       embedPath = embedPath,\n",
    "                       weightsPath = weightsPath,\n",
    "                       wordEmPath = wordEmPath)\n",
    "filterAll, y_train = model.dataPre()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3  Data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given all data pathes, we merge all data together using **inner join**. All actions we took for our data.\n",
    "- 1.Drop lines with Null GMDN name, Material Description and Prdha Description.\n",
    "- 2.Uniform all Prdha code. (e.g Some Prdha codes are not 18 digits. We use a function to make all codes 18 digits.)\n",
    "\n",
    "**Extra Explaination:** \n",
    "\n",
    "We only select limited features from the table: \n",
    "\n",
    "- Production Dimension : Breit, Brgew, Hoehe, Laeng, Volum, Ntgew (width, length, height...)\n",
    "- Text Fields : Material Description, Gmdnptdefinition (long), Gmdnptname (short), Minor_name, Major_name, Material (all text)\n",
    "- Digits : Ean11 (GTIN), Unspsc, Prdha\n",
    "- Others : Zzwerks (location), Material_top3 (first three chars of material description) \n",
    "\n",
    "**Related Modules:**\n",
    "* dataPre : Create and prepare all required data\n",
    "* prdha_zero : Change all prdha code to 18 digits. (fill 0 in the beginning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Part of the data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Breit                                                                   0\n",
       "Brgew                                                                   1\n",
       "Hoehe                                                                   0\n",
       "Laeng                                                                   0\n",
       "Volum                                                                   0\n",
       "Zzwerks                                                              CA02\n",
       "Ntgew                                                                   1\n",
       "Material Description                                     NORIAN DRILLABLE\n",
       "Material                                                   07.704.003S-US\n",
       "Ean11                                                          1.0887e+13\n",
       "Gmdnptdefinition        A sterile bioabsorbable device made of synthet...\n",
       "Gmdnptname                                 Bone matrix implant, synthetic\n",
       "Unspsc                                                           42291501\n",
       "Prdha                                                  055078377837047824\n",
       "Minor_name                                                   Biomaterials\n",
       "Major_name                                                 Synthes Trauma\n",
       "Material_top3                                                         07.\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterAll.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gmdnptdefinition</th>\n",
       "      <th>Gmdnptname</th>\n",
       "      <th>Unspsc</th>\n",
       "      <th>Prdha</th>\n",
       "      <th>Minor_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A sterile bioabsorbable device made of synthet...</td>\n",
       "      <td>Bone matrix implant, synthetic</td>\n",
       "      <td>42291501</td>\n",
       "      <td>055078377837047824</td>\n",
       "      <td>Biomaterials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A sterile bioabsorbable device made of synthet...</td>\n",
       "      <td>Bone matrix implant, synthetic</td>\n",
       "      <td>42291501</td>\n",
       "      <td>055078377837047824</td>\n",
       "      <td>Biomaterials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A sterile bioabsorbable device made of synthet...</td>\n",
       "      <td>Bone matrix implant, synthetic</td>\n",
       "      <td>42291501</td>\n",
       "      <td>055078377837047824</td>\n",
       "      <td>Biomaterials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A sterile bioabsorbable device made of synthet...</td>\n",
       "      <td>Bone matrix implant, synthetic</td>\n",
       "      <td>42291501</td>\n",
       "      <td>055078377837047824</td>\n",
       "      <td>Biomaterials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A sterile bioabsorbable device made of synthet...</td>\n",
       "      <td>Bone matrix implant, synthetic</td>\n",
       "      <td>42291501</td>\n",
       "      <td>055078377837047824</td>\n",
       "      <td>Biomaterials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Gmdnptdefinition  \\\n",
       "0  A sterile bioabsorbable device made of synthet...   \n",
       "1  A sterile bioabsorbable device made of synthet...   \n",
       "2  A sterile bioabsorbable device made of synthet...   \n",
       "3  A sterile bioabsorbable device made of synthet...   \n",
       "4  A sterile bioabsorbable device made of synthet...   \n",
       "\n",
       "                       Gmdnptname    Unspsc               Prdha    Minor_name  \n",
       "0  Bone matrix implant, synthetic  42291501  055078377837047824  Biomaterials  \n",
       "1  Bone matrix implant, synthetic  42291501  055078377837047824  Biomaterials  \n",
       "2  Bone matrix implant, synthetic  42291501  055078377837047824  Biomaterials  \n",
       "3  Bone matrix implant, synthetic  42291501  055078377837047824  Biomaterials  \n",
       "4  Bone matrix implant, synthetic  42291501  055078377837047824  Biomaterials  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filterAll.iloc[:,10:15].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is very complicated. Generally speaking, it can be divided into two parts.\n",
    "- Model Training\n",
    "- Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.1 Modeling Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, all modules have been encapsulated into the final model **stagingFeature**.\n",
    "\n",
    "**Step 1: Sentence to Words**\n",
    "- At first, Keras text mining tool will parse each sentence and then split them into words.\n",
    "- Related Modules : Token, TokenInput, tokenSide\n",
    "- We have **3** models according to different text. This step will create different words of bags. The number of each bag is different because some text has more different words (more information). \n",
    "\n",
    "**Step 2: Word Embedding**\n",
    "- Next, Fasttest will change these words into matrix. The embedding matrix will be the first layer for neutral network.\n",
    "- Related Modules : wordEmbedding\n",
    "\n",
    "**Step 3: RNN**\n",
    "- We use very complex techniques in the modeling part. For Natural Language Processing, we use RNN GRU. For detailed model part, see example below.\n",
    "\n",
    "**Step 4 : Encapsulation**\n",
    "- All these functions are encapsulated into one module. To simplify this training process, changing some parameters can train the model.\n",
    "- Advanced Technique: we use **autoencoder** for informaiton extraction and compression. As a result, the output will be the inner layer of RNN.\n",
    "- Related Module : modelTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: GMDN**\n",
    "- Use pre-cleaned data : filterAl(x), y_train(y)\n",
    "- Text Type : gmdn\n",
    "- Pre-trained : False (use new data to train the model)\n",
    "- Summay : True (show training framework)\n",
    "- epoches : juse epoch (50-60s per epoch, more epoches more accurate)\n",
    "- Input : False (this is designed for interface)\n",
    "\n",
    "**Important**\n",
    "- Input : Determine if you will use new input data (x).\n",
    "- Pre_trained : Determine if you need to train the model.\n",
    "\n",
    "If choosing to train, folder \"preTrainedWordEmbed\" will have a new file with beginning \"New\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         210000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 160)         182880    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 73)                7373      \n",
      "=================================================================\n",
      "Total params: 416,353\n",
      "Trainable params: 416,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16560 samples, validate on 4141 samples\n",
      "Epoch 1/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 3.5732 - acc: 0.1476 - val_loss: 3.1100 - val_acc: 0.1599\n",
      "Epoch 2/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.9088 - acc: 0.2254 - val_loss: 2.6479 - val_acc: 0.2941\n",
      "Epoch 3/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.4195 - acc: 0.4385 - val_loss: 2.2515 - val_acc: 0.4477\n",
      "Epoch 4/20\n",
      "16560/16560 [==============================] - 57s 3ms/step - loss: 2.1079 - acc: 0.4867 - val_loss: 1.9715 - val_acc: 0.5506\n",
      "Epoch 5/20\n",
      "16560/16560 [==============================] - 56s 3ms/step - loss: 1.8683 - acc: 0.5115 - val_loss: 1.8076 - val_acc: 0.5291\n",
      "Epoch 6/20\n",
      "16560/16560 [==============================] - 55s 3ms/step - loss: 1.7100 - acc: 0.5300 - val_loss: 1.6532 - val_acc: 0.5211\n",
      "Epoch 7/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.5815 - acc: 0.5437 - val_loss: 1.5362 - val_acc: 0.5438\n",
      "Epoch 8/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 1.4684 - acc: 0.5567 - val_loss: 1.4322 - val_acc: 0.5574\n",
      "Epoch 9/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 1.3751 - acc: 0.5861 - val_loss: 1.3520 - val_acc: 0.5887\n",
      "Epoch 10/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.2972 - acc: 0.6185 - val_loss: 1.2805 - val_acc: 0.6085\n",
      "Epoch 11/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.2257 - acc: 0.6534 - val_loss: 1.2075 - val_acc: 0.6907\n",
      "Epoch 12/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.1403 - acc: 0.6954 - val_loss: 1.1067 - val_acc: 0.7278\n",
      "Epoch 13/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.0454 - acc: 0.7284 - val_loss: 1.0093 - val_acc: 0.7375\n",
      "Epoch 14/20\n",
      "16560/16560 [==============================] - 60s 4ms/step - loss: 0.9549 - acc: 0.7384 - val_loss: 0.9211 - val_acc: 0.7580\n",
      "Epoch 15/20\n",
      "16560/16560 [==============================] - 63s 4ms/step - loss: 0.8790 - acc: 0.7540 - val_loss: 0.8571 - val_acc: 0.7597\n",
      "Epoch 16/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 0.8210 - acc: 0.7706 - val_loss: 0.8005 - val_acc: 0.7860\n",
      "Epoch 17/20\n",
      "16560/16560 [==============================] - 65s 4ms/step - loss: 0.7710 - acc: 0.7819 - val_loss: 0.7540 - val_acc: 0.7856\n",
      "Epoch 18/20\n",
      "16560/16560 [==============================] - 63s 4ms/step - loss: 0.7233 - acc: 0.7862 - val_loss: 0.7062 - val_acc: 0.7868\n",
      "Epoch 19/20\n",
      "16560/16560 [==============================] - 66s 4ms/step - loss: 0.6786 - acc: 0.7924 - val_loss: 0.6628 - val_acc: 0.7914\n",
      "Epoch 20/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 0.6364 - acc: 0.8081 - val_loss: 0.6194 - val_acc: 0.8211\n",
      "20701/20701 [==============================] - 24s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "gmdn_output = model.modelTrain(filterAll,y_train,'gmdn',\n",
    "                               Pre_trained=False,Summary=True,epochs = 20,Input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Gmdn already has accuracy 80.81 % ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 300)         1200000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 160)         182880    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               16100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 73)                7373      \n",
      "=================================================================\n",
      "Total params: 1,406,353\n",
      "Trainable params: 1,406,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16560 samples, validate on 4141 samples\n",
      "Epoch 1/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 3.4155 - acc: 0.1906 - val_loss: 3.0961 - val_acc: 0.2270\n",
      "Epoch 2/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 3.0115 - acc: 0.2411 - val_loss: 2.8257 - val_acc: 0.3050\n",
      "Epoch 3/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 2.5580 - acc: 0.3843 - val_loss: 2.3698 - val_acc: 0.4320\n",
      "Epoch 4/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 2.2738 - acc: 0.4478 - val_loss: 2.2502 - val_acc: 0.4784\n",
      "Epoch 5/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.1711 - acc: 0.4719 - val_loss: 2.1530 - val_acc: 0.5170\n",
      "Epoch 6/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.0460 - acc: 0.5056 - val_loss: 1.9801 - val_acc: 0.5156\n",
      "Epoch 7/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 1.8932 - acc: 0.5360 - val_loss: 1.8604 - val_acc: 0.5518\n",
      "Epoch 8/20\n",
      "16560/16560 [==============================] - 63s 4ms/step - loss: 1.7523 - acc: 0.5747 - val_loss: 1.7132 - val_acc: 0.5907\n",
      "Epoch 9/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.6164 - acc: 0.5987 - val_loss: 1.6001 - val_acc: 0.5982\n",
      "Epoch 10/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.5001 - acc: 0.6268 - val_loss: 1.4940 - val_acc: 0.6281\n",
      "Epoch 11/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.3974 - acc: 0.6524 - val_loss: 1.3909 - val_acc: 0.6692\n",
      "Epoch 12/20\n",
      "16560/16560 [==============================] - 60s 4ms/step - loss: 1.2950 - acc: 0.6760 - val_loss: 1.2890 - val_acc: 0.6706\n",
      "Epoch 13/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.1937 - acc: 0.6844 - val_loss: 1.1936 - val_acc: 0.6853\n",
      "Epoch 14/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.1030 - acc: 0.7082 - val_loss: 1.1157 - val_acc: 0.7039\n",
      "Epoch 15/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 1.0244 - acc: 0.7270 - val_loss: 1.0455 - val_acc: 0.7305\n",
      "Epoch 16/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 0.9475 - acc: 0.7604 - val_loss: 0.9785 - val_acc: 0.7469\n",
      "Epoch 17/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 0.8819 - acc: 0.7762 - val_loss: 0.9239 - val_acc: 0.7728\n",
      "Epoch 18/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 0.8186 - acc: 0.7995 - val_loss: 0.8650 - val_acc: 0.7959\n",
      "Epoch 19/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 0.7631 - acc: 0.8117 - val_loss: 0.8221 - val_acc: 0.8071\n",
      "Epoch 20/20\n",
      "16560/16560 [==============================] - 60s 4ms/step - loss: 0.7143 - acc: 0.8221 - val_loss: 0.7698 - val_acc: 0.8167\n",
      "20701/20701 [==============================] - 24s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "material_output = model.modelTrain(filterAll,y_train,'material',\n",
    "                               Pre_trained=False,Summary=True,epochs = 20,Input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Gmdn already has accuracy 80.88 % ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         300000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 160)         182880    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               32200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 73)                14673     \n",
      "=================================================================\n",
      "Total params: 529,753\n",
      "Trainable params: 529,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16560 samples, validate on 4141 samples\n",
      "Epoch 1/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 3.3703 - acc: 0.1662 - val_loss: 3.0847 - val_acc: 0.2270\n",
      "Epoch 2/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.9940 - acc: 0.2398 - val_loss: 2.8819 - val_acc: 0.2618\n",
      "Epoch 3/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.7154 - acc: 0.2874 - val_loss: 2.5681 - val_acc: 0.3115\n",
      "Epoch 4/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.5282 - acc: 0.3235 - val_loss: 2.5058 - val_acc: 0.3253\n",
      "Epoch 5/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.4788 - acc: 0.3284 - val_loss: 2.4693 - val_acc: 0.3236\n",
      "Epoch 6/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.4628 - acc: 0.3275 - val_loss: 2.4581 - val_acc: 0.3258\n",
      "Epoch 7/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.4001 - acc: 0.3501 - val_loss: 2.3916 - val_acc: 0.3432\n",
      "Epoch 8/20\n",
      "16560/16560 [==============================] - 59s 4ms/step - loss: 2.3645 - acc: 0.3511 - val_loss: 2.3734 - val_acc: 0.3463\n",
      "Epoch 9/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.3445 - acc: 0.3530 - val_loss: 2.3508 - val_acc: 0.3463\n",
      "Epoch 10/20\n",
      "16560/16560 [==============================] - 63s 4ms/step - loss: 2.3024 - acc: 0.3562 - val_loss: 2.2846 - val_acc: 0.3881\n",
      "Epoch 11/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.2268 - acc: 0.3922 - val_loss: 2.2046 - val_acc: 0.3895\n",
      "Epoch 12/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.1366 - acc: 0.4051 - val_loss: 2.0954 - val_acc: 0.4074\n",
      "Epoch 13/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 2.0445 - acc: 0.4165 - val_loss: 2.0183 - val_acc: 0.4202\n",
      "Epoch 14/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 1.9722 - acc: 0.4482 - val_loss: 1.9333 - val_acc: 0.4586\n",
      "Epoch 15/20\n",
      "16560/16560 [==============================] - 61s 4ms/step - loss: 1.8969 - acc: 0.4568 - val_loss: 1.8712 - val_acc: 0.4530\n",
      "Epoch 16/20\n",
      "16560/16560 [==============================] - 60s 4ms/step - loss: 1.8528 - acc: 0.4579 - val_loss: 1.8400 - val_acc: 0.4637\n",
      "Epoch 17/20\n",
      "16560/16560 [==============================] - 55s 3ms/step - loss: 1.8253 - acc: 0.4630 - val_loss: 1.8144 - val_acc: 0.4562\n",
      "Epoch 18/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.8031 - acc: 0.4638 - val_loss: 1.7888 - val_acc: 0.4714\n",
      "Epoch 19/20\n",
      "16560/16560 [==============================] - 63s 4ms/step - loss: 1.7838 - acc: 0.4683 - val_loss: 1.7728 - val_acc: 0.4716\n",
      "Epoch 20/20\n",
      "16560/16560 [==============================] - 62s 4ms/step - loss: 1.7678 - acc: 0.4699 - val_loss: 1.7580 - val_acc: 0.4690\n",
      "20701/20701 [==============================] - 24s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "prdha_output = model.modelTrain(filterAll,y_train,'prdha',\n",
    "                               Pre_trained=False,Summary=True,epochs = 20,Input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Gmdn already has accuracy 46.95 % ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20701, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmdn_output.shape # (row,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.2 Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained model is also provied in this part so that pre-trained parameters can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_gmdn_output, pre_material_output, pre_prdha_output = model.stagingFeature(filterAll,y_train,Pre_trained=True,Summary=False,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.3 Merge All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This part is very simple. Just merge all previous word embedding matrix and other features such product dimensions, locations and other information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_featuresReady = model.FeatureMerge(pre_gmdn_output,pre_material_output,pre_prdha_output,filterAll)\n",
    "featuresReady = model.FeatureMerge(gmdn_output,material_output,prdha_output,filterAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breit</th>\n",
       "      <th>Brgew</th>\n",
       "      <th>Hoehe</th>\n",
       "      <th>Laeng</th>\n",
       "      <th>Volum</th>\n",
       "      <th>Location_BR06</th>\n",
       "      <th>Location_BR08</th>\n",
       "      <th>Location_CA02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Breit  Brgew  Hoehe  Laeng  Volum  Location_BR06  Location_BR08  \\\n",
       "0    0.0    1.0    0.0    0.0    0.0              0              0   \n",
       "1    0.0    1.0    0.0    0.0    0.0              0              0   \n",
       "2    0.0    1.0    0.0    0.0    0.0              0              0   \n",
       "\n",
       "   Location_CA02  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_featuresReady.iloc[:3,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20701, 417)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_featuresReady.shape # (row,column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.4 Boosting Tree Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With all previous features, final model is XGBoost Boosting Tree\n",
    "- Here, we only show 1 round with accuracy 98.07% (35 rounds will reach 99.95%.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.017391\ttest-merror:0.019804\n"
     ]
    }
   ],
   "source": [
    "finalPrediction = model.finalPrediction(featuresReady, y_train, num_round=1,preTrained=False,InputTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_finalPrediction = model.finalPrediction(featuresReady, y_train, num_round=30,preTrained=True,InputTest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ..., 72., 72., 72.], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_finalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 72, 72, 72])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix : Reference Materials\n",
    "- Books: Flask Web Development: Developing Web Applications with Python \n",
    "- Tech : plotly, Dash"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
